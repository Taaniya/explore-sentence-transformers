{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiGANeLE8K4U"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "f5wWCJrK8RXG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = AutoModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "gnjzy9Ik8Zvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Dark matter is completely invisible, emits no light or energy and thus cannot be detected by conventional sensors and detectors\""
      ],
      "metadata": {
        "id": "R3YEZLfg8mas"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = bert_tokenizer.tokenize(text)\n",
        "print(len(tokens))\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_Yaei8Q9aHu",
        "outputId": "03104183-ebd9-45bf-b294-891ed4804f33"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n",
            "['dark', 'matter', 'is', 'completely', 'invisible', ',', 'emi', '##ts', 'no', 'light', 'or', 'energy', 'and', 'thus', 'cannot', 'be', 'detected', 'by', 'conventional', 'sensors', 'and', 'detectors']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert_tokenizer.convert_tokens_to_ids(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F14hyPZb9fap",
        "outputId": "225d7dac-646e-4866-b06f-0a52be010bac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2601, 3043, 2003, 3294, 8841, 1010, 12495, 3215, 2053, 2422, 2030, 2943, 1998, 2947, 3685, 2022, 11156, 2011, 7511, 13907, 1998, 25971]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get encoded input by Bert tokenizer\n",
        "encoded_input = bert_tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "encoded_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVA21QYi9tJM",
        "outputId": "62dd4eaf-f755-4f4c-f03a-5b2dc20f4d8d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2601,  3043,  2003,  3294,  8841,  1010, 12495,  3215,  2053,\n",
              "          2422,  2030,  2943,  1998,  2947,  3685,  2022, 11156,  2011,  7511,\n",
              "         13907,  1998, 25971,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 1st and last token IDs added to the end of original token ID list are the special [CLS] & [SEP] tokens"
      ],
      "metadata": {
        "id": "8frwyDF3-JEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert_tokenizer.convert_ids_to_tokens([ 101, 102]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v30z8lSX-DLc",
        "outputId": "13ff679b-3bf0-4fd8-dd41-d589fa37112b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute token embeddings\n",
        "\n",
        "with torch.no_grad():\n",
        "    model_output = bert_model(**encoded_input)"
      ],
      "metadata": {
        "id": "LIvshBJS-a0r"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_output.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4f3mCY6-7Kn",
        "outputId": "a96d7369-8463-4ee8-ef0e-f7d58119d892"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['last_hidden_state', 'pooler_output'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the raw output of Bert model without any specific head on top.\n",
        "\n",
        "`last hidden state` is the hidden state representation in the last layer for each token in the sequence including the special tokens.\n",
        "\n",
        "`pooler_output` is the last layer hidden state representation corresponding to the [CLS] token after processing with a linear layer with tanh activation function using weights learnt during training on next sentence prediction task during pre-training.\n",
        "\n",
        "More details on BertModel in documentation [here](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel)"
      ],
      "metadata": {
        "id": "nTPtSv5e_UBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_output['last_hidden_state'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1BcP4CK-_JK",
        "outputId": "b9ccdba1-d3f2-4ec5-fdc4-fd2a3f85e9bb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 24, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_output['pooler_output'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2IchexT_9QO",
        "outputId": "ecf8e33b-e495-41fe-d321-189703a6cd2c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### References\n",
        "* https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
        "* [BERT Paper, 2018](https://arxiv.org/pdf/1810.04805.pdf)"
      ],
      "metadata": {
        "id": "yvMid8cr_D0w"
      }
    }
  ]
}